{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0485594",
   "metadata": {},
   "source": [
    "# Quiz Concepts Practice Lab: Mastering Regression Methods\n",
    "\n",
    "**Date:** November 12, 2025  \n",
    "**Module:** 2 - Supervised Machine Learning: Regression  \n",
    "**Purpose:** Hands-on practice for quiz concepts you got wrong\n",
    "\n",
    "---\n",
    "\n",
    "## üìö What This Lab Covers\n",
    "\n",
    "This lab addresses the specific concepts from your quiz:\n",
    "\n",
    "1. **Classical vs. Modern ML Methods** - Understanding the difference\n",
    "2. **Multiple Linear Regression** - When to use it\n",
    "3. **OLS Limitations** - Why outliers are problematic\n",
    "4. **OLS as MSE Minimizer** - How it actually works\n",
    "5. **Overfitting vs. Underfitting** - High-degree polynomial problems\n",
    "\n",
    "---\n",
    "\n",
    "## üîó Key Concepts (Glossary References)\n",
    "\n",
    "**See:** `notes/glossary-module-2.md` for detailed definitions\n",
    "\n",
    "- **Linear Regression** - Classical statistical method using OLS\n",
    "- **Polynomial Regression** - Still classical! Uses OLS on transformed features\n",
    "- **Random Forest** - Modern ML ensemble method\n",
    "- **Multiple Linear Regression** - 2+ independent variables\n",
    "- **Ordinary Least Squares (OLS)** - Method minimizing MSE\n",
    "- **Mean Squared Error (MSE)** - Cost function OLS minimizes\n",
    "- **Gradient Descent** - Iterative optimization (NOT the same as OLS)\n",
    "- **Overfitting** - Model memorizes training data including noise\n",
    "- **Underfitting** - Model too simple to capture patterns\n",
    "- **Outlier** - Data point far from others, affects OLS heavily\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this lab, you will:\n",
    "- ‚úì Distinguish classical (OLS-based) from modern ML methods\n",
    "- ‚úì Implement and compare simple vs. multiple linear regression\n",
    "- ‚úì Visualize why OLS is sensitive to outliers\n",
    "- ‚úì Demonstrate that OLS minimizes MSE mathematically\n",
    "- ‚úì Show overfitting with high-degree polynomials\n",
    "- ‚úì Make correct choices for your next quiz attempt!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413dbb96",
   "metadata": {},
   "source": [
    "## üì¶ Setup: Run Me First!\n",
    "\n",
    "Import all necessary libraries and set random seeds for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b7e65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import numerical computation library (see glossary: NumPy)\n",
    "import numpy as np\n",
    "\n",
    "# Import data manipulation library (see glossary: Pandas)\n",
    "import pandas as pd\n",
    "\n",
    "# Import visualization libraries (see glossary: Matplotlib, Seaborn)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Import sklearn for machine learning models\n",
    "from sklearn.linear_model import LinearRegression  # Classical OLS-based method\n",
    "from sklearn.ensemble import RandomForestRegressor  # Modern ML ensemble method\n",
    "from sklearn.preprocessing import PolynomialFeatures  # For polynomial regression\n",
    "from sklearn.model_selection import train_test_split  # Split data into train/test\n",
    "from sklearn.metrics import mean_squared_error, r2_score  # Evaluation metrics\n",
    "\n",
    "# Set random seed for reproducibility (see glossary: Random State)\n",
    "np.random.seed(42)  # Ensures same random numbers every time\n",
    "\n",
    "# Configure matplotlib for better plots\n",
    "plt.style.use('seaborn-v0_8-darkgrid')  # Use seaborn style for nicer plots\n",
    "plt.rcParams['figure.figsize'] = (10, 6)  # Set default figure size\n",
    "plt.rcParams['font.size'] = 11  # Set default font size\n",
    "\n",
    "# Print versions for reproducibility\n",
    "print(f\"NumPy version: {np.__version__}\")  # Show NumPy version\n",
    "print(f\"Pandas version: {pd.__version__}\")  # Show Pandas version\n",
    "print(\"‚úì Setup complete! Ready to learn.\")  # Confirm setup successful"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63bb9c84",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚ùì Quiz Question 1: Classical vs. Modern ML Methods\n",
    "\n",
    "**Question:** Which of the following regression methods is a modern machine learning technique?\n",
    "\n",
    "**Your Answer:** Polynomial regression ‚ùå  \n",
    "**Correct Answer:** Random forest regression ‚úÖ\n",
    "\n",
    "**Why This Matters:** Understanding the distinction between classical statistical methods (OLS-based) and modern ML methods (algorithmic/ensemble) is crucial for choosing the right tool.\n",
    "\n",
    "### Classical Methods (Pre-1990s, Statistical)\n",
    "- Linear Regression\n",
    "- Polynomial Regression\n",
    "- Simple Linear Regression\n",
    "- **Characteristic:** Use OLS (closed-form solution), no hyperparameters\n",
    "\n",
    "### Modern ML Methods (1990s+, Computer Science)\n",
    "- Random Forest\n",
    "- Neural Networks\n",
    "- Support Vector Machines\n",
    "- Gradient Boosting\n",
    "- **Characteristic:** Iterative training, hyperparameters, no closed-form solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483cb187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample dataset for regression (see glossary: Synthetic Data)\n",
    "np.random.seed(42)  # Set seed for reproducibility\n",
    "X = np.linspace(0, 10, 50).reshape(-1, 1)  # Create 50 evenly spaced points from 0 to 10\n",
    "y = 2 * X.flatten() + 1 + np.random.normal(0, 1, 50)  # Linear relationship: y = 2x + 1 + noise\n",
    "\n",
    "# CLASSICAL METHOD: Linear Regression (uses OLS under the hood)\n",
    "classical_model = LinearRegression()  # Initialize linear regression model (see glossary: Linear Regression)\n",
    "classical_model.fit(X, y)  # Train model using OLS formula: Œ∏ = (X^T X)^-1 X^T y\n",
    "y_pred_classical = classical_model.predict(X)  # Make predictions using learned parameters\n",
    "\n",
    "# MODERN ML METHOD: Random Forest (ensemble of decision trees)\n",
    "modern_model = RandomForestRegressor(n_estimators=100, random_state=42)  # Initialize with 100 trees (see glossary: Random Forest)\n",
    "modern_model.fit(X, y)  # Train 100 decision trees iteratively\n",
    "y_pred_modern = modern_model.predict(X)  # Aggregate predictions from all trees\n",
    "\n",
    "# Calculate performance metrics (see glossary: MSE, R-squared)\n",
    "mse_classical = mean_squared_error(y, y_pred_classical)  # Classical MSE\n",
    "mse_modern = mean_squared_error(y, y_pred_modern)  # Modern MSE\n",
    "r2_classical = r2_score(y, y_pred_classical)  # Classical R¬≤\n",
    "r2_modern = r2_score(y, y_pred_modern)  # Modern R¬≤\n",
    "\n",
    "# Print comparison\n",
    "print(\"=\"*60)\n",
    "print(\"CLASSICAL vs. MODERN ML METHODS COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nüìä Linear Regression (Classical/OLS-based):\")\n",
    "print(f\"   MSE: {mse_classical:.3f}\")\n",
    "print(f\"   R¬≤: {r2_classical:.3f}\")\n",
    "print(f\"   Training: Closed-form solution (instant)\")\n",
    "print(f\"   Hyperparameters: None\")\n",
    "print(f\"\\nüå≤ Random Forest (Modern ML):\")\n",
    "print(f\"   MSE: {mse_modern:.3f}\")\n",
    "print(f\"   R¬≤: {r2_modern:.3f}\")\n",
    "print(f\"   Training: Iterative (100 trees)\")\n",
    "print(f\"   Hyperparameters: n_estimators, max_depth, min_samples_split, etc.\")\n",
    "print(\"\\n‚úÖ Key Takeaway: Random Forest is MODERN ML, not classical!\")\n",
    "print(\"   Polynomial Regression is still CLASSICAL (uses OLS)!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6473132",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚ùì Quiz Question 3: OLS Sensitivity to Outliers\n",
    "\n",
    "**Question:** Why is OLS regression's accuracy for complex data sets limited?\n",
    "\n",
    "**Your Answer:** Requires extensive tuning ‚ùå  \n",
    "**Correct Answer:** May inaccurately weigh outliers, resulting in skewed outputs ‚úÖ\n",
    "\n",
    "**Why This Matters:** OLS minimizes **squared** errors, so large errors (outliers) get amplified. A single outlier can dramatically skew the best-fit line.\n",
    "\n",
    "### Demonstration: How Outliers Affect OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72acbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create clean house price dataset (see glossary: Dataset)\n",
    "np.random.seed(42)  # Set seed for reproducibility\n",
    "sizes_clean = np.array([1000, 1500, 2000, 2500, 3000, 3500]).reshape(-1, 1)  # House sizes in sq ft\n",
    "prices_clean = 50 + 0.15 * sizes_clean.flatten() + np.random.normal(0, 10, 6)  # Prices: $50k + $150/sqft + noise\n",
    "\n",
    "# Create dataset WITH outlier (see glossary: Outlier)\n",
    "sizes_outlier = np.append(sizes_clean, [[5000]], axis=0)  # Add mansion (5000 sq ft)\n",
    "prices_outlier = np.append(prices_clean, [600])  # Add outlier price ($600k - way too high!)\n",
    "\n",
    "# Fit OLS without outlier (see glossary: OLS)\n",
    "model_clean = LinearRegression()  # Initialize model\n",
    "model_clean.fit(sizes_clean, prices_clean)  # Train on clean data using OLS: Œ∏ = (X^T X)^-1 X^T y\n",
    "pred_clean = model_clean.predict(sizes_clean)  # Predict with clean model\n",
    "\n",
    "# Fit OLS WITH outlier\n",
    "model_outlier = LinearRegression()  # Initialize model\n",
    "model_outlier.fit(sizes_outlier, prices_outlier)  # Train on data with outlier\n",
    "pred_outlier_all = model_outlier.predict(sizes_outlier)  # Predict with outlier-affected model\n",
    "pred_outlier_clean = model_outlier.predict(sizes_clean)  # Predict clean sizes with outlier-affected model\n",
    "\n",
    "# Calculate error contributions (see glossary: MSE, Sum of Squared Residuals)\n",
    "error_clean_point = (prices_clean[0] - pred_clean[0])**2  # Squared error for normal house\n",
    "error_outlier_point = (prices_outlier[-1] - pred_outlier_all[-1])**2  # Squared error for outlier\n",
    "\n",
    "# Visualize the impact\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))  # Create two side-by-side plots\n",
    "\n",
    "# Plot WITHOUT outlier\n",
    "ax1.scatter(sizes_clean, prices_clean, c='blue', s=100, alpha=0.6, label='Normal houses')  # Plot data points\n",
    "ax1.plot(sizes_clean, pred_clean, 'r-', linewidth=2, label='OLS best-fit (no outlier)')  # Plot regression line\n",
    "ax1.set_xlabel('House Size (sq ft)')  # Label x-axis\n",
    "ax1.set_ylabel('Price ($1000s)')  # Label y-axis\n",
    "ax1.set_title('OLS Regression WITHOUT Outlier\\n(Good Fit)', fontsize=14, fontweight='bold')  # Add title\n",
    "ax1.legend()  # Show legend\n",
    "ax1.grid(True, alpha=0.3)  # Add grid\n",
    "\n",
    "# Plot WITH outlier\n",
    "ax2.scatter(sizes_clean, prices_clean, c='blue', s=100, alpha=0.6, label='Normal houses')  # Plot normal houses\n",
    "ax2.scatter([5000], [600], c='red', s=200, marker='*', label='OUTLIER (mansion)', zorder=5)  # Plot outlier\n",
    "ax2.plot(sizes_clean, pred_clean, 'g--', linewidth=2, alpha=0.5, label='Original fit')  # Show original fit\n",
    "ax2.plot(sizes_outlier, pred_outlier_all, 'r-', linewidth=2, label='OLS with outlier (SKEWED)')  # Plot skewed fit\n",
    "ax2.set_xlabel('House Size (sq ft)')  # Label x-axis\n",
    "ax2.set_ylabel('Price ($1000s)')  # Label y-axis\n",
    "ax2.set_title('OLS Regression WITH Outlier\\n(Line Pulled Toward Outlier!)', fontsize=14, fontweight='bold')  # Add title\n",
    "ax2.legend()  # Show legend\n",
    "ax2.grid(True, alpha=0.3)  # Add grid\n",
    "\n",
    "plt.tight_layout()  # Adjust spacing\n",
    "plt.show()  # Display plots\n",
    "\n",
    "# Print analysis\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"OLS OUTLIER SENSITIVITY ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nüìä Model WITHOUT outlier:\")\n",
    "print(f\"   Slope: ${model_clean.coef_[0]:.2f} per sq ft\")\n",
    "print(f\"   Intercept: ${model_clean.intercept_:.2f}k\")\n",
    "print(f\"   Prediction for 2000 sq ft: ${model_clean.predict([[2000]])[0]:.2f}k\")\n",
    "print(f\"\\nüìä Model WITH outlier:\")\n",
    "print(f\"   Slope: ${model_outlier.coef_[0]:.2f} per sq ft  ‚Üê CHANGED!\")\n",
    "print(f\"   Intercept: ${model_outlier.intercept_:.2f}k  ‚Üê CHANGED!\")\n",
    "print(f\"   Prediction for 2000 sq ft: ${model_outlier.predict([[2000]])[0]:.2f}k  ‚Üê OVERESTIMATED!\")\n",
    "print(f\"\\n‚ö†Ô∏è  Impact of ONE outlier:\")\n",
    "difference = model_outlier.predict([[2000]])[0] - model_clean.predict([[2000]])[0]\n",
    "print(f\"   Single outlier changed prediction by ${difference:.2f}k ({(difference/model_clean.predict([[2000]])[0]*100):.1f}%)\")\n",
    "print(f\"\\n‚ùå Why OLS is limited:\")\n",
    "print(f\"   - Squared errors amplify outliers: Error¬≤ for outlier = {error_outlier_point:.1f}\")\n",
    "print(f\"   - Normal house error¬≤ = {error_clean_point:.1f}\")\n",
    "print(f\"   - Outlier contributes {(error_outlier_point/error_clean_point):.1f}√ó more to cost!\")\n",
    "print(f\"   - OLS bends line to minimize this huge squared error\")\n",
    "print(f\"\\n‚úÖ Key Takeaway: OLS DOES NOT require tuning (that's its strength!)\")\n",
    "print(f\"   But OLS IS sensitive to outliers (that's its weakness!)\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e022fd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚ùì Quiz Question 4: OLS Minimizes MSE\n",
    "\n",
    "**Question:** What model estimates coefficients by minimizing MSE?\n",
    "\n",
    "**Your Answer:** Gradient descent ‚ùå  \n",
    "**Correct Answer:** Ordinary least squares ‚úÖ\n",
    "\n",
    "**Why This Matters:** OLS **IS** the method that minimizes MSE using a closed-form formula. Gradient descent is an *optimization approach* (iterative) used when closed-form solutions don't exist.\n",
    "\n",
    "### Mathematical Proof: OLS Minimizes MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a120319e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate that OLS formula DIRECTLY minimizes MSE (see glossary: OLS, MSE)\n",
    "np.random.seed(42)  # Set seed\n",
    "X_simple = np.array([[1], [2], [3], [4], [5]])  # Simple X values\n",
    "y_simple = np.array([2.1, 3.9, 6.2, 7.9, 10.1])  # Simple y values (roughly y = 2x)\n",
    "\n",
    "# METHOD 1: Use sklearn LinearRegression (which uses OLS under the hood)\n",
    "sklearn_model = LinearRegression()  # Initialize model\n",
    "sklearn_model.fit(X_simple, y_simple)  # Train using OLS formula\n",
    "theta_sklearn = [sklearn_model.intercept_, sklearn_model.coef_[0]]  # Extract parameters\n",
    "\n",
    "# METHOD 2: Calculate OLS formula manually (see glossary: OLS Formula)\n",
    "# OLS formula: Œ∏ = (X^T X)^(-1) X^T y\n",
    "X_with_intercept = np.c_[np.ones(X_simple.shape[0]), X_simple]  # Add column of 1's for intercept: [1, x]\n",
    "XtX = X_with_intercept.T @ X_with_intercept  # Calculate X^T X (matrix multiplication)\n",
    "XtX_inv = np.linalg.inv(XtX)  # Calculate (X^T X)^(-1) (matrix inversion)\n",
    "Xty = X_with_intercept.T @ y_simple  # Calculate X^T y (matrix multiplication)\n",
    "theta_manual = XtX_inv @ Xty  # Final OLS formula: Œ∏ = (X^T X)^(-1) X^T y\n",
    "\n",
    "# Calculate MSE for both methods (see glossary: MSE)\n",
    "y_pred_sklearn = sklearn_model.predict(X_simple)  # Predictions from sklearn\n",
    "y_pred_manual = X_with_intercept @ theta_manual  # Predictions from manual OLS\n",
    "mse_sklearn = np.mean((y_simple - y_pred_sklearn)**2)  # MSE from sklearn\n",
    "mse_manual = np.mean((y_simple - y_pred_manual)**2)  # MSE from manual OLS\n",
    "\n",
    "# Try some random parameters to show OLS gives minimum MSE\n",
    "random_mses = []  # List to store MSE values for random parameters\n",
    "for i in range(100):  # Try 100 random parameter combinations\n",
    "    random_intercept = np.random.uniform(-5, 5)  # Random intercept\n",
    "    random_slope = np.random.uniform(0, 5)  # Random slope\n",
    "    y_pred_random = random_intercept + random_slope * X_simple.flatten()  # Random predictions\n",
    "    mse_random = np.mean((y_simple - y_pred_random)**2)  # MSE for random parameters\n",
    "    random_mses.append(mse_random)  # Store MSE\n",
    "\n",
    "# Print results\n",
    "print(\"=\"*70)\n",
    "print(\"PROOF: OLS MINIMIZES MSE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nüìê Sklearn LinearRegression (uses OLS internally):\")\n",
    "print(f\"   Œ∏‚ÇÄ (intercept) = {theta_sklearn[0]:.4f}\")\n",
    "print(f\"   Œ∏‚ÇÅ (slope) = {theta_sklearn[1]:.4f}\")\n",
    "print(f\"   MSE = {mse_sklearn:.6f}\")\n",
    "print(f\"\\nüìê Manual OLS Formula: Œ∏ = (X^T X)^(-1) X^T y\")\n",
    "print(f\"   Œ∏‚ÇÄ (intercept) = {theta_manual[0]:.4f}\")\n",
    "print(f\"   Œ∏‚ÇÅ (slope) = {theta_manual[1]:.4f}\")\n",
    "print(f\"   MSE = {mse_manual:.6f}\")\n",
    "print(f\"\\n‚úÖ Both methods give IDENTICAL results!\")\n",
    "print(f\"   Difference: {abs(mse_sklearn - mse_manual):.10f} (essentially zero)\")\n",
    "print(f\"\\nüìä Comparison with 100 random parameter choices:\")\n",
    "print(f\"   OLS MSE: {mse_sklearn:.6f}\")\n",
    "print(f\"   Minimum random MSE: {min(random_mses):.6f}\")\n",
    "print(f\"   Average random MSE: {np.mean(random_mses):.6f}\")\n",
    "print(f\"   Maximum random MSE: {max(random_mses):.6f}\")\n",
    "print(f\"\\n‚úÖ Key Takeaway: OLS finds the MINIMUM MSE!\")\n",
    "print(f\"   - OLS is THE METHOD that minimizes MSE\")\n",
    "print(f\"   - Gradient descent is an OPTIMIZATION APPROACH (iterative)\")\n",
    "print(f\"   - OLS uses closed-form formula (one calculation, exact)\")\n",
    "print(f\"   - Gradient descent uses iterations (multiple calculations, approximate)\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4407ecb0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚ùì Quiz Question 5: Overfitting - Memorizing Noise\n",
    "\n",
    "**Question:** What issue occurs when high-degree polynomial memorizes random noise?\n",
    "\n",
    "**Your Answer:** Underfitting ‚ùå  \n",
    "**Correct Answer:** Overfitting ‚úÖ\n",
    "\n",
    "**Why This Matters:** \n",
    "- **Overfitting** = Model TOO COMPLEX, MEMORIZES training data (including noise)\n",
    "- **Underfitting** = Model TOO SIMPLE, MISSES the pattern\n",
    "\n",
    "**Key Word:** \"MEMORIZES\" = OVERFITTING!\n",
    "\n",
    "### Visual Demonstration: Underfitting vs. Good Fit vs. Overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1418b525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset with cubic relationship + noise (see glossary: Synthetic Data, Noise)\n",
    "np.random.seed(42)  # Set seed\n",
    "X_poly = np.linspace(-3, 3, 20).reshape(-1, 1)  # 20 points from -3 to 3\n",
    "y_true = X_poly.flatten()**3 - 5*X_poly.flatten()  # True cubic relationship\n",
    "noise = np.random.normal(0, 5, 20)  # Random noise (see glossary: Noise)\n",
    "y_poly = y_true + noise  # Observed data = true relationship + noise\n",
    "\n",
    "# Split into train and test (see glossary: Train/Test Split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_poly, y_poly, test_size=0.3, random_state=42)\n",
    "\n",
    "# Generate points for smooth plotting\n",
    "X_plot = np.linspace(-3, 3, 100).reshape(-1, 1)  # 100 points for smooth curves\n",
    "y_plot_true = X_plot.flatten()**3 - 5*X_plot.flatten()  # True relationship for reference\n",
    "\n",
    "# UNDERFITTING: Degree-1 (Linear) - TOO SIMPLE (see glossary: Underfitting)\n",
    "poly_underfit = PolynomialFeatures(degree=1)  # Transform features to degree-1 (just X)\n",
    "X_train_underfit = poly_underfit.fit_transform(X_train)  # Transform training data\n",
    "X_test_underfit = poly_underfit.transform(X_test)  # Transform test data\n",
    "X_plot_underfit = poly_underfit.transform(X_plot)  # Transform plotting data\n",
    "model_underfit = LinearRegression()  # Initialize linear model\n",
    "model_underfit.fit(X_train_underfit, y_train)  # Train on degree-1 features\n",
    "y_pred_underfit = model_underfit.predict(X_plot_underfit)  # Predict for plotting\n",
    "train_mse_underfit = mean_squared_error(y_train, model_underfit.predict(X_train_underfit))  # Training MSE\n",
    "test_mse_underfit = mean_squared_error(y_test, model_underfit.predict(X_test_underfit))  # Test MSE\n",
    "\n",
    "# GOOD FIT: Degree-3 (Cubic) - JUST RIGHT (see glossary: Polynomial Regression)\n",
    "poly_good = PolynomialFeatures(degree=3)  # Transform to degree-3 (X, X¬≤, X¬≥)\n",
    "X_train_good = poly_good.fit_transform(X_train)  # Transform training data\n",
    "X_test_good = poly_good.transform(X_test)  # Transform test data\n",
    "X_plot_good = poly_good.transform(X_plot)  # Transform plotting data\n",
    "model_good = LinearRegression()  # Initialize model\n",
    "model_good.fit(X_train_good, y_train)  # Train on degree-3 features\n",
    "y_pred_good = model_good.predict(X_plot_good)  # Predict for plotting\n",
    "train_mse_good = mean_squared_error(y_train, model_good.predict(X_train_good))  # Training MSE\n",
    "test_mse_good = mean_squared_error(y_test, model_good.predict(X_test_good))  # Test MSE\n",
    "\n",
    "# OVERFITTING: Degree-15 (Very High) - TOO COMPLEX (see glossary: Overfitting)\n",
    "poly_overfit = PolynomialFeatures(degree=15)  # Transform to degree-15 (X, X¬≤, ..., X¬π‚Åµ)\n",
    "X_train_overfit = poly_overfit.fit_transform(X_train)  # Transform training data\n",
    "X_test_overfit = poly_overfit.transform(X_test)  # Transform test data\n",
    "X_plot_overfit = poly_overfit.transform(X_plot)  # Transform plotting data\n",
    "model_overfit = LinearRegression()  # Initialize model\n",
    "model_overfit.fit(X_train_overfit, y_train)  # Train on degree-15 features (MEMORIZES!)\n",
    "y_pred_overfit = model_overfit.predict(X_plot_overfit)  # Predict for plotting\n",
    "train_mse_overfit = mean_squared_error(y_train, model_overfit.predict(X_train_overfit))  # Training MSE (will be very low!)\n",
    "test_mse_overfit = mean_squared_error(y_test, model_overfit.predict(X_test_overfit))  # Test MSE (will be high!)\n",
    "\n",
    "# Create visualization showing all three cases\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))  # Create three side-by-side plots\n",
    "\n",
    "# Plot 1: UNDERFITTING\n",
    "axes[0].scatter(X_train, y_train, c='blue', s=50, alpha=0.6, label='Training data', zorder=3)  # Training points\n",
    "axes[0].scatter(X_test, y_test, c='green', s=50, alpha=0.6, marker='s', label='Test data', zorder=3)  # Test points\n",
    "axes[0].plot(X_plot, y_plot_true, 'k--', alpha=0.3, label='True relationship', linewidth=1)  # True curve\n",
    "axes[0].plot(X_plot, y_pred_underfit, 'r-', linewidth=3, label='Linear fit')  # Model prediction\n",
    "axes[0].set_title(f'UNDERFITTING (Degree=1)\\nTrain MSE: {train_mse_underfit:.1f} | Test MSE: {test_mse_underfit:.1f}', \n",
    "                  fontsize=12, fontweight='bold')  # Title with MSE\n",
    "axes[0].set_xlabel('X')  # X label\n",
    "axes[0].set_ylabel('y')  # Y label\n",
    "axes[0].legend()  # Legend\n",
    "axes[0].grid(True, alpha=0.3)  # Grid\n",
    "axes[0].set_ylim(-30, 30)  # Set y-axis limits\n",
    "\n",
    "# Plot 2: GOOD FIT\n",
    "axes[1].scatter(X_train, y_train, c='blue', s=50, alpha=0.6, label='Training data', zorder=3)  # Training points\n",
    "axes[1].scatter(X_test, y_test, c='green', s=50, alpha=0.6, marker='s', label='Test data', zorder=3)  # Test points\n",
    "axes[1].plot(X_plot, y_plot_true, 'k--', alpha=0.3, label='True relationship', linewidth=1)  # True curve\n",
    "axes[1].plot(X_plot, y_pred_good, 'g-', linewidth=3, label='Cubic fit')  # Model prediction\n",
    "axes[1].set_title(f'GOOD FIT (Degree=3) ‚úì\\nTrain MSE: {train_mse_good:.1f} | Test MSE: {test_mse_good:.1f}', \n",
    "                  fontsize=12, fontweight='bold', color='green')  # Title with MSE\n",
    "axes[1].set_xlabel('X')  # X label\n",
    "axes[1].set_ylabel('y')  # Y label\n",
    "axes[1].legend()  # Legend\n",
    "axes[1].grid(True, alpha=0.3)  # Grid\n",
    "axes[1].set_ylim(-30, 30)  # Set y-axis limits\n",
    "\n",
    "# Plot 3: OVERFITTING\n",
    "axes[2].scatter(X_train, y_train, c='blue', s=50, alpha=0.6, label='Training data', zorder=3)  # Training points\n",
    "axes[2].scatter(X_test, y_test, c='green', s=50, alpha=0.6, marker='s', label='Test data', zorder=3)  # Test points\n",
    "axes[2].plot(X_plot, y_plot_true, 'k--', alpha=0.3, label='True relationship', linewidth=1)  # True curve\n",
    "axes[2].plot(X_plot, y_pred_overfit, 'm-', linewidth=3, label='Degree-15 fit')  # Model prediction (wiggly!)\n",
    "axes[2].set_title(f'OVERFITTING (Degree=15)\\nTrain MSE: {train_mse_overfit:.1f} | Test MSE: {test_mse_overfit:.1f}', \n",
    "                  fontsize=12, fontweight='bold', color='red')  # Title with MSE\n",
    "axes[2].set_xlabel('X')  # X label\n",
    "axes[2].set_ylabel('y')  # Y label\n",
    "axes[2].legend()  # Legend\n",
    "axes[2].grid(True, alpha=0.3)  # Grid\n",
    "axes[2].set_ylim(-30, 30)  # Set y-axis limits\n",
    "\n",
    "plt.tight_layout()  # Adjust spacing\n",
    "plt.show()  # Display\n",
    "\n",
    "# Print comprehensive analysis\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"OVERFITTING vs. UNDERFITTING ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\n‚ùå UNDERFITTING (Degree=1 - Linear):\")\n",
    "print(f\"   Model: TOO SIMPLE\")\n",
    "print(f\"   Training MSE: {train_mse_underfit:.2f} (HIGH - can't fit training data well)\")\n",
    "print(f\"   Test MSE: {test_mse_underfit:.2f} (HIGH - can't generalize either)\")\n",
    "print(f\"   Problem: Misses the cubic pattern, just draws straight line\")\n",
    "print(f\"   Symptom: High bias (systematic errors)\")\n",
    "print(f\"\\n‚úÖ GOOD FIT (Degree=3 - Cubic):\")\n",
    "print(f\"   Model: JUST RIGHT\")\n",
    "print(f\"   Training MSE: {train_mse_good:.2f} (LOW - fits training data well)\")\n",
    "print(f\"   Test MSE: {test_mse_good:.2f} (LOW - generalizes well)\")\n",
    "print(f\"   Solution: Matches the true cubic relationship!\")\n",
    "print(f\"   Balance: Low bias, low variance\")\n",
    "print(f\"\\n‚ùå OVERFITTING (Degree=15 - Very High Polynomial):\")\n",
    "print(f\"   Model: TOO COMPLEX\")\n",
    "print(f\"   Training MSE: {train_mse_overfit:.2f} (VERY LOW - memorizes training data!)\")\n",
    "print(f\"   Test MSE: {test_mse_overfit:.2f} (VERY HIGH - fails on new data!)\")\n",
    "print(f\"   Problem: Wiggly line passes through every training point, including NOISE\")\n",
    "print(f\"   Symptom: High variance (predictions change wildly with small data changes)\")\n",
    "print(f\"\\nüéØ Quiz Question Keywords:\")\n",
    "print(f\"   - 'HIGH-DEGREE polynomial' ‚Üí Degree=15 (overfitting candidate)\")\n",
    "print(f\"   - 'MEMORIZES random noise' ‚Üí OVERFITTING!\")\n",
    "print(f\"   - NOT underfitting (that's when model is too simple)\")\n",
    "print(f\"\\nüìä Performance Ratio:\")\n",
    "print(f\"   Underfitting: Test/Train = {test_mse_underfit/train_mse_underfit:.2f} (both bad)\")\n",
    "print(f\"   Good Fit: Test/Train = {test_mse_good/train_mse_good:.2f} (close to 1.0)\")\n",
    "print(f\"   Overfitting: Test/Train = {test_mse_overfit/train_mse_overfit:.2f} (test >>> train)\")\n",
    "print(f\"\\n‚úÖ Key Takeaway: Degree-15 polynomial OVERFITS by memorizing noise!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137e4238",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Quiz Answer Quick Reference\n",
    "\n",
    "### For Your Next Attempt:\n",
    "\n",
    "| Question | Correct Answer | Key Concept |\n",
    "|----------|----------------|-------------|\n",
    "| **Q1: Modern ML method?** | **Random Forest** | Classical = OLS-based (Linear, Polynomial); Modern = algorithmic (Random Forest, Neural Nets) |\n",
    "| **Q3: OLS limitation?** | **Sensitive to outliers** | Squared errors amplify outliers; OLS does NOT need tuning (that's its strength!) |\n",
    "| **Q4: Minimizes MSE?** | **OLS** | OLS is THE METHOD; Gradient Descent is an optimization approach |\n",
    "| **Q5: Memorizes noise?** | **Overfitting** | Memorizing = OVERFITTING (too complex); Underfitting = too simple |\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ What You Should Remember\n",
    "\n",
    "### Classical vs. Modern\n",
    "- **If it uses OLS ‚Üí Classical** (Linear, Polynomial, Simple Linear)\n",
    "- **If it's algorithmic/ensemble ‚Üí Modern** (Random Forest, Neural Networks, SVM)\n",
    "\n",
    "### OLS Characteristics\n",
    "- **‚úì Strength:** No tuning, fast, exact solution\n",
    "- **‚úó Weakness:** Sensitive to outliers (squared errors)\n",
    "\n",
    "### OLS vs. Gradient Descent\n",
    "- **OLS:** Method/Formula - Œ∏ = (X^T X)^(-1) X^T y (one calculation)\n",
    "- **Gradient Descent:** Optimization approach (iterative updates)\n",
    "\n",
    "### Overfitting vs. Underfitting\n",
    "- **Underfitting:** Model too simple, high training error, high test error\n",
    "- **Overfitting:** Model too complex, memorizes training data, very low training error, very high test error\n",
    "\n",
    "**Decision Tree:**\n",
    "```\n",
    "Does model fit training data well?\n",
    "‚îú‚îÄ NO ‚Üí UNDERFITTING (increase complexity)\n",
    "‚îî‚îÄ YES ‚Üí Does it fit test data well?\n",
    "           ‚îú‚îÄ YES ‚Üí GOOD FIT! ‚úì\n",
    "           ‚îî‚îÄ NO ‚Üí OVERFITTING (decrease complexity / add regularization)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üìù Practice Exercises\n",
    "\n",
    "Try these to cement your understanding:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d427d684",
   "metadata": {},
   "source": [
    "### Exercise 1: Classify the Methods\n",
    "\n",
    "For each method below, classify as **CLASSICAL** or **MODERN ML**:\n",
    "\n",
    "1. Simple Linear Regression - ?\n",
    "2. Support Vector Machine - ?\n",
    "3. Polynomial Regression (degree 5) - ?\n",
    "4. Gradient Boosting Machine - ?\n",
    "5. Multiple Linear Regression - ?\n",
    "6. Neural Network - ?\n",
    "7. k-Nearest Neighbors - ?\n",
    "\n",
    "<details>\n",
    "<summary>Click for Answers</summary>\n",
    "\n",
    "1. Simple Linear Regression - **CLASSICAL** (uses OLS)\n",
    "2. Support Vector Machine - **MODERN ML** (iterative, hyperparameters)\n",
    "3. Polynomial Regression (degree 5) - **CLASSICAL** (still uses OLS!)\n",
    "4. Gradient Boosting Machine - **MODERN ML** (ensemble, iterative)\n",
    "5. Multiple Linear Regression - **CLASSICAL** (uses OLS)\n",
    "6. Neural Network - **MODERN ML** (backpropagation, many hyperparameters)\n",
    "7. k-Nearest Neighbors - **MODERN ML** (algorithmic, hyperparameter k)\n",
    "\n",
    "**Key:** If it mentions \"linear\" or \"polynomial\" ‚Üí probably classical!\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### Exercise 2: Identify the Problem\n",
    "\n",
    "For each scenario, identify **UNDERFITTING**, **GOOD FIT**, or **OVERFITTING**:\n",
    "\n",
    "1. Training R¬≤ = 0.45, Test R¬≤ = 0.42 - ?\n",
    "2. Training R¬≤ = 0.99, Test R¬≤ = 0.25 - ?\n",
    "3. Training R¬≤ = 0.88, Test R¬≤ = 0.85 - ?\n",
    "4. Degree-20 polynomial on 15 data points - ?\n",
    "5. Linear model on exponential data - ?\n",
    "\n",
    "<details>\n",
    "<summary>Click for Answers</summary>\n",
    "\n",
    "1. Training R¬≤ = 0.45, Test R¬≤ = 0.42 - **UNDERFITTING** (both low)\n",
    "2. Training R¬≤ = 0.99, Test R¬≤ = 0.25 - **OVERFITTING** (train high, test low)\n",
    "3. Training R¬≤ = 0.88, Test R¬≤ = 0.85 - **GOOD FIT** (both high, close together)\n",
    "4. Degree-20 polynomial on 15 data points - **OVERFITTING** (too complex for data size)\n",
    "5. Linear model on exponential data - **UNDERFITTING** (too simple for pattern)\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### Exercise 3: OLS True or False\n",
    "\n",
    "1. OLS requires extensive hyperparameter tuning - ?\n",
    "2. OLS is sensitive to outliers - ?\n",
    "3. OLS uses gradient descent - ?\n",
    "4. OLS minimizes Mean Squared Error - ?\n",
    "5. Polynomial regression uses gradient descent - ?\n",
    "\n",
    "<details>\n",
    "<summary>Click for Answers</summary>\n",
    "\n",
    "1. **FALSE** - OLS requires NO tuning (closed-form solution)\n",
    "2. **TRUE** - Squared errors amplify outliers\n",
    "3. **FALSE** - OLS uses closed-form formula, not gradient descent\n",
    "4. **TRUE** - OLS directly minimizes MSE via calculus\n",
    "5. **FALSE** - Polynomial regression uses OLS (closed-form), not gradient descent\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "## üéì Final Tips for Quiz Success\n",
    "\n",
    "### Before Answering, Ask Yourself:\n",
    "\n",
    "**For method classification:**\n",
    "- Does it use OLS? ‚Üí Classical\n",
    "- Does it require iterative training? ‚Üí Modern ML\n",
    "\n",
    "**For OLS questions:**\n",
    "- Remember: OLS = METHOD that minimizes MSE\n",
    "- Gradient descent = OPTIMIZATION APPROACH (different thing!)\n",
    "- Strength = no tuning; Weakness = outlier sensitivity\n",
    "\n",
    "**For overfitting/underfitting:**\n",
    "- Look for keywords: \"memorize\", \"high-degree\", \"complex\"\n",
    "- Memorizing = OVERFITTING\n",
    "- Too simple = UNDERFITTING\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Further Reading\n",
    "\n",
    "- **Study Guide:** `notes/module-2/05-quiz-concepts-study-guide.md`\n",
    "- **Lesson Notes:** `notes/module-2/04-polynomial-and-nonlinear-regression.md`\n",
    "- **Glossary:** `notes/glossary-module-2.md`\n",
    "\n",
    "---\n",
    "\n",
    "**Good luck on your next quiz attempt! You've got this! üöÄ**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
